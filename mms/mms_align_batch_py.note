## 3/20/25
## Claude recommends this as a way to process in smaller batches
## to reduce GPU memory requirements

import sys
import json
import torch
import torchaudio
from torchaudio.pipelines import MMS_FA as bundle

# Set batch size for word list processing
batch_size = 50  # Number of words/segments to process at once

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = bundle.get_model(with_star=False)
model.to(device)
tokenizer = bundle.get_tokenizer()
aligner = bundle.get_aligner()

for line in sys.stdin:
    torch.cuda.empty_cache()
    inp = json.loads(line)
    waveform, sample_rate = torchaudio.load(inp["audio_file"])
    assert sample_rate == bundle.sample_rate

    # Process audio through model once
    with torch.inference_mode():
        emission, _ = model(waveform.to(device))

    num_frames = emission.size(1)
    ratio = waveform.size(1) / num_frames / sample_rate

    # Process words in batches if the list is large
    words = inp["words"]
    all_token_spans = []

    for i in range(0, len(words), batch_size):
        word_batch = words[i:i+batch_size]

        # Process this batch of words
        with torch.inference_mode():
            token_spans_batch = aligner(emission[0], tokenizer(word_batch))

        all_token_spans.extend(token_spans_batch)

    # Format the results
    chapter = []
    for spans in all_token_spans:
        word = []
        for token in spans:
            char = [token.token, token.start, token.end, token.score]
            word.append(char)
        chapter.append(word)

    result = {
       'ratio': ratio,
       'dictionary': bundle.get_dict(),
       'tokens': chapter
    }

    output = json.dumps(result)
    sys.stdout.write(output)
    sys.stdout.write("\n")
    sys.stdout.flush()